{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664f2f67-d217-4a9c-b794-fa7fecd115a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.12.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # disable warnings\n",
    "\n",
    "from os import getcwd\n",
    "from os.path import join, abspath, pardir, exists\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle, json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plotly\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "# scipy\n",
    "from scipy.stats import ttest_ind, chi2_contingency, boxcox, skew\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "# sklearn libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, LocalOutlierFactor\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, MissingIndicator\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline, Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.compose import make_column_selector, make_column_transformer, make_column_transformer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer # enable experimental imputer\n",
    "from sklearn.impute import IterativeImputer               # sample imputation\n",
    "from sklearn import preprocessing                         # encoders, transformations\n",
    "from sklearn.model_selection import cross_validate        # cross-validation, model evaluation\n",
    "from sklearn.model_selection import GridSearchCV          # hyper-parameter tuning\n",
    "from sklearn.linear_model import LogisticRegression       # logistic regression model\n",
    "from sklearn.svm import SVC                               # support vector machine model\n",
    "from sklearn.neighbors import KNeighborsClassifier        # k-nearest neighbours model\n",
    "from sklearn.ensemble import GradientBoostingClassifier   # gradient boosting model\n",
    "from sklearn.ensemble import VotingClassifier             # voting ensemble model\n",
    "from sklearn.ensemble import StackingClassifier           # stacking ensemble model\n",
    "\n",
    "\n",
    "# statsmodel\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "\n",
    "# IPython\n",
    "from IPython.display import display\n",
    "from IPython.core.interactiveshell import InteractiveShell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83b4df",
   "metadata": {},
   "source": [
    "##### Config settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4894fe-4fc0-4580-b2df-275bbdfb82bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = abspath(join(join(getcwd(), pardir), pardir))\n",
    "data_dir = join(parent_dir, \"data\")\n",
    "model_dir = join(parent_dir, \"models\")\n",
    "data_file = join(data_dir, \"test.csv\")\n",
    "\n",
    "# For IPython\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # To show all output after each cell execution (instead of the last output)\n",
    "\n",
    "# For pandas\n",
    "\n",
    "pd.options.display.max_columns = 200 # display upto 200 columns (instead of default 20)\n",
    "pd.options.display.max_rows = 200 # display upto 200 rows (instead of default 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627281a6",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46177fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save model as a pickle file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "def load_model(file_path: str):\n",
    "    \"\"\"\n",
    "    Load model from a pickle file\n",
    "    \"\"\"\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "def dataframe_to_csv(df: pd.DataFrame, file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Save dataframe as .csv file\n",
    "    \"\"\"\n",
    "    df.to_csv(file_path, index=False)\n",
    "\n",
    "def plot_distribution(data, bins, title, xlabel, ylabel):\n",
    "    \"\"\"\n",
    "    Plot distribution functions\n",
    "    \"\"\"\n",
    "    ax = sns.distplot(\n",
    "        data,\n",
    "        bins=bins,\n",
    "        hist_kws={\n",
    "            \"linewidth\": 1,\n",
    "            'edgecolor': 'black',\n",
    "            'alpha': 1.0\n",
    "            },\n",
    "        kde=False\n",
    "    )\n",
    "    _ = ax.set_title(title)\n",
    "    _ = ax.set_xlabel(xlabel)\n",
    "    _ = ax.set_ylabel(ylabel)\n",
    "\n",
    "def plot_relationship(x, y, title, xlabel, ylabel):\n",
    "    \"\"\"\n",
    "    Plot relationship between two features\n",
    "    \"\"\"\n",
    "    ax = sns.barplot(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        orient='h'\n",
    "    )\n",
    "    _ = ax.set_title(title)\n",
    "    _ = ax.set_xlabel(xlabel)\n",
    "    _ = ax.set_ylabel(ylabel)\n",
    "\n",
    "def print_moments(title, feature):\n",
    "    \"\"\"\n",
    "    Print a feature's mean, standard deviation, skewness and kurtosis\n",
    "    \"\"\"\n",
    "    print(title)\n",
    "    print('Mean: '+'{:>18.2f}'.format(feature.mean()))\n",
    "    print('Standard deviation: '+'{:.2f}'.format(feature.std()))\n",
    "    print('Skewness: '+'{:>14.2f}'.format(feature.skew()))\n",
    "    print('Kurtosis: '+'{:>14.2f}'.format(feature.kurtosis()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea30025",
   "metadata": {},
   "source": [
    "#### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c0d863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iid</th>\n",
       "      <th>gender</th>\n",
       "      <th>wave</th>\n",
       "      <th>position</th>\n",
       "      <th>order</th>\n",
       "      <th>pid</th>\n",
       "      <th>age_o</th>\n",
       "      <th>race_o</th>\n",
       "      <th>pf_o_att</th>\n",
       "      <th>pf_o_sin</th>\n",
       "      <th>pf_o_int</th>\n",
       "      <th>pf_o_fun</th>\n",
       "      <th>pf_o_amb</th>\n",
       "      <th>pf_o_sha</th>\n",
       "      <th>dec_o</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>like_o</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>met_o</th>\n",
       "      <th>age</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>race</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>goal</th>\n",
       "      <th>date</th>\n",
       "      <th>go_out</th>\n",
       "      <th>career_c</th>\n",
       "      <th>sports</th>\n",
       "      <th>tvsports</th>\n",
       "      <th>exercise</th>\n",
       "      <th>dining</th>\n",
       "      <th>museums</th>\n",
       "      <th>art</th>\n",
       "      <th>hiking</th>\n",
       "      <th>gaming</th>\n",
       "      <th>clubbing</th>\n",
       "      <th>reading</th>\n",
       "      <th>tv</th>\n",
       "      <th>theater</th>\n",
       "      <th>movies</th>\n",
       "      <th>concerts</th>\n",
       "      <th>music</th>\n",
       "      <th>shopping</th>\n",
       "      <th>yoga</th>\n",
       "      <th>exphappy</th>\n",
       "      <th>expnum</th>\n",
       "      <th>attr1_1</th>\n",
       "      <th>sinc1_1</th>\n",
       "      <th>intel1_1</th>\n",
       "      <th>fun1_1</th>\n",
       "      <th>amb1_1</th>\n",
       "      <th>shar1_1</th>\n",
       "      <th>attr3_1</th>\n",
       "      <th>sinc3_1</th>\n",
       "      <th>fun3_1</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>dec</th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>amb</th>\n",
       "      <th>shar</th>\n",
       "      <th>like</th>\n",
       "      <th>prob</th>\n",
       "      <th>met</th>\n",
       "      <th>match_es</th>\n",
       "      <th>satis_2</th>\n",
       "      <th>length</th>\n",
       "      <th>numdat_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>False</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>True</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>False</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>40.00</td>\n",
       "      <td>15.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>False</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>361</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>352.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>False</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>True</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>30.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>153</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>173.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.37</td>\n",
       "      <td>18.37</td>\n",
       "      <td>20.41</td>\n",
       "      <td>20.41</td>\n",
       "      <td>16.33</td>\n",
       "      <td>6.12</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>28.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.76</td>\n",
       "      <td>19.51</td>\n",
       "      <td>19.51</td>\n",
       "      <td>14.63</td>\n",
       "      <td>19.51</td>\n",
       "      <td>17.07</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>46.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>35.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>22.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>True</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iid  gender  wave  position  order    pid  age_o  race_o  pf_o_att  \\\n",
       "0   50    True     2        12     13   25.0   25.0     2.0     20.00   \n",
       "1   53    True     2        19      7   33.0   30.0     2.0     30.00   \n",
       "2  361    True    14        12      9  352.0   29.0     6.0     20.00   \n",
       "3  153   False     7        11     12  173.0   22.0     2.0     18.37   \n",
       "4   35   False     2        17     16   46.0   26.0     2.0     20.00   \n",
       "\n",
       "   pf_o_sin  pf_o_int  pf_o_fun  pf_o_amb  pf_o_sha  dec_o  attr_o  sinc_o  \\\n",
       "0     20.00     15.00     15.00     15.00     15.00  False     9.0     8.0   \n",
       "1     20.00     30.00     10.00      0.00     10.00  False     8.0     7.0   \n",
       "2     20.00     20.00     10.00     10.00     20.00  False     6.0     6.0   \n",
       "3     18.37     20.41     20.41     16.33      6.12  False     5.0     5.0   \n",
       "4     35.00     20.00     10.00     10.00      5.00   True     8.0     8.0   \n",
       "\n",
       "   intel_o  fun_o  amb_o  shar_o  like_o  prob_o  met_o   age  field_cd  race  \\\n",
       "0      9.0    8.0   10.0     7.0     8.0     7.0   True  27.0       1.0   1.0   \n",
       "1      5.0    6.0    7.0     4.0     6.0     3.0   True  28.0       4.0   2.0   \n",
       "2      7.0    6.0    6.0     5.0     6.0     6.0   True  34.0       1.0   2.0   \n",
       "3      5.0    5.0    5.0     5.0     5.0     3.0   True  28.0       7.0   4.0   \n",
       "4      8.0    6.0    9.0     8.0     8.0     5.0   True  25.0       2.0   2.0   \n",
       "\n",
       "   imprace  imprelig  goal  date  go_out  career_c  sports  tvsports  \\\n",
       "0      4.0       4.0   6.0   6.0     2.0       1.0    10.0      10.0   \n",
       "1      2.0       2.0   2.0   6.0     2.0       4.0     8.0       4.0   \n",
       "2      5.0       5.0   2.0   6.0     6.0       1.0     3.0       1.0   \n",
       "3      1.0       1.0   2.0   6.0     3.0       2.0     8.0       5.0   \n",
       "4      9.0       7.0   1.0   6.0     3.0       7.0     9.0       7.0   \n",
       "\n",
       "   exercise  dining  museums   art  hiking  gaming  clubbing  reading   tv  \\\n",
       "0       3.0     8.0      5.0   3.0     4.0     3.0      10.0      5.0  6.0   \n",
       "1       3.0    10.0      9.0   8.0     8.0     4.0       4.0      9.0  5.0   \n",
       "2       3.0     5.0      9.0  10.0     3.0     3.0       4.0     10.0  6.0   \n",
       "3       7.0     7.0      7.0   6.0     6.0     5.0       7.0     10.0  6.0   \n",
       "4       9.0     8.0      8.0   8.0     5.0     6.0       9.0      7.0  9.0   \n",
       "\n",
       "   theater  movies  concerts  music  shopping  yoga  exphappy  expnum  \\\n",
       "0      8.0    10.0       7.0   10.0       6.0   2.0       5.0     7.0   \n",
       "1      7.0     9.0       9.0    9.0       7.0   3.0       6.0     9.0   \n",
       "2      8.0     9.0       7.0    7.0       3.0   2.0       5.0     NaN   \n",
       "3      7.0     9.0       7.0    7.0       8.0   5.0       6.0     NaN   \n",
       "4      8.0     8.0       7.0    7.0       8.0   3.0       7.0     8.0   \n",
       "\n",
       "   attr1_1  sinc1_1  intel1_1  fun1_1  amb1_1  shar1_1  attr3_1  sinc3_1  \\\n",
       "0    35.00    10.00     20.00   15.00   10.00    10.00      8.0      9.0   \n",
       "1    40.00    15.00     20.00   10.00    5.00    10.00      8.0      9.0   \n",
       "2    30.00    20.00     30.00    5.00    5.00    10.00      7.0      7.0   \n",
       "3     9.76    19.51     19.51   14.63   19.51    17.07      7.0      7.0   \n",
       "4    20.00    23.00     23.00   22.00    7.00     5.00      8.0      9.0   \n",
       "\n",
       "   fun3_1  intel3_1  amb3_1    dec  attr  sinc  intel  fun  amb  shar  like  \\\n",
       "0     9.0      10.0     8.0   True   7.0   NaN    NaN  NaN  NaN   NaN   8.0   \n",
       "1     8.0       8.0     8.0  False   7.0   9.0    9.0  7.0  9.0   7.0   7.0   \n",
       "2     5.0       7.0     9.0  False   6.0  10.0    6.0  6.0  6.0   4.0   5.0   \n",
       "3     6.0       7.0     6.0  False   5.0   5.0    6.0  6.0  NaN   NaN   5.0   \n",
       "4     9.0       9.0     7.0   True   8.0   8.0    9.0  9.0  8.0  10.0   8.0   \n",
       "\n",
       "   prob  met  match_es  satis_2  length  numdat_2  \n",
       "0   7.0  2.0       5.0      6.0     1.0       2.0  \n",
       "1   8.0  2.0       8.0      8.0     3.0       3.0  \n",
       "2   2.0  0.0       4.0      7.0     3.0       3.0  \n",
       "3   5.0  0.0       4.0      5.0     1.0       3.0  \n",
       "4   8.0  2.0      10.0      6.0     1.0       2.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_file, encoding= 'ISO-8859-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499547fe",
   "metadata": {},
   "source": [
    "## Test Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a3839",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "075aa23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gb = load_model(join(model_dir, \"clf_gb.pkl\"))\n",
    "clf_knn = load_model(join(model_dir, \"clf_knn.pkl\"))\n",
    "clf_logistic_regression = load_model(join(model_dir, \"clf_logistic_regression.pkl\"))\n",
    "clf_stacking = load_model(join(model_dir, \"clf_stacking.pkl\"))\n",
    "clf_svc = load_model(join(model_dir, \"clf_svc.pkl\"))\n",
    "clf_voting = load_model(join(model_dir, \"clf_voting.pkl\"))\n",
    "col_trans = load_model(join(model_dir, \"col_trans.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6446811",
   "metadata": {},
   "source": [
    "##### Basic checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64442b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expnum</th>\n",
       "      <th>match_es</th>\n",
       "      <th>shar_o</th>\n",
       "      <th>shar</th>\n",
       "      <th>numdat_2</th>\n",
       "      <th>length</th>\n",
       "      <th>satis_2</th>\n",
       "      <th>amb_o</th>\n",
       "      <th>amb</th>\n",
       "      <th>fun</th>\n",
       "      <th>fun_o</th>\n",
       "      <th>met</th>\n",
       "      <th>prob_o</th>\n",
       "      <th>prob</th>\n",
       "      <th>sinc_o</th>\n",
       "      <th>intel_o</th>\n",
       "      <th>intel</th>\n",
       "      <th>like_o</th>\n",
       "      <th>like</th>\n",
       "      <th>sinc</th>\n",
       "      <th>attr</th>\n",
       "      <th>attr_o</th>\n",
       "      <th>pf_o_sha</th>\n",
       "      <th>age_o</th>\n",
       "      <th>pf_o_amb</th>\n",
       "      <th>pf_o_fun</th>\n",
       "      <th>attr3_1</th>\n",
       "      <th>sinc3_1</th>\n",
       "      <th>fun3_1</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>race_o</th>\n",
       "      <th>pf_o_att</th>\n",
       "      <th>pf_o_sin</th>\n",
       "      <th>pf_o_int</th>\n",
       "      <th>career_c</th>\n",
       "      <th>exphappy</th>\n",
       "      <th>date</th>\n",
       "      <th>shar1_1</th>\n",
       "      <th>age</th>\n",
       "      <th>amb1_1</th>\n",
       "      <th>fun1_1</th>\n",
       "      <th>intel1_1</th>\n",
       "      <th>sinc1_1</th>\n",
       "      <th>attr1_1</th>\n",
       "      <th>shopping</th>\n",
       "      <th>yoga</th>\n",
       "      <th>art</th>\n",
       "      <th>music</th>\n",
       "      <th>museums</th>\n",
       "      <th>imprace</th>\n",
       "      <th>imprelig</th>\n",
       "      <th>goal</th>\n",
       "      <th>go_out</th>\n",
       "      <th>concerts</th>\n",
       "      <th>tvsports</th>\n",
       "      <th>exercise</th>\n",
       "      <th>dining</th>\n",
       "      <th>sports</th>\n",
       "      <th>gaming</th>\n",
       "      <th>clubbing</th>\n",
       "      <th>reading</th>\n",
       "      <th>tv</th>\n",
       "      <th>theater</th>\n",
       "      <th>movies</th>\n",
       "      <th>hiking</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>race</th>\n",
       "      <th>pid</th>\n",
       "      <th>wave</th>\n",
       "      <th>position</th>\n",
       "      <th>order</th>\n",
       "      <th>gender</th>\n",
       "      <th>dec_o</th>\n",
       "      <th>met_o</th>\n",
       "      <th>dec</th>\n",
       "      <th>iid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>missing %</th>\n",
       "      <td>77.724741</td>\n",
       "      <td>14.638027</td>\n",
       "      <td>12.967383</td>\n",
       "      <td>12.728719</td>\n",
       "      <td>11.296738</td>\n",
       "      <td>11.137629</td>\n",
       "      <td>11.137629</td>\n",
       "      <td>8.512331</td>\n",
       "      <td>8.035004</td>\n",
       "      <td>4.375497</td>\n",
       "      <td>4.295943</td>\n",
       "      <td>3.89817</td>\n",
       "      <td>3.579952</td>\n",
       "      <td>3.18218</td>\n",
       "      <td>3.18218</td>\n",
       "      <td>2.943516</td>\n",
       "      <td>2.863962</td>\n",
       "      <td>2.704853</td>\n",
       "      <td>2.625298</td>\n",
       "      <td>2.545744</td>\n",
       "      <td>1.988862</td>\n",
       "      <td>1.909308</td>\n",
       "      <td>1.909308</td>\n",
       "      <td>1.511535</td>\n",
       "      <td>1.431981</td>\n",
       "      <td>1.352426</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.272872</td>\n",
       "      <td>1.034208</td>\n",
       "      <td>0.875099</td>\n",
       "      <td>0.795545</td>\n",
       "      <td>0.795545</td>\n",
       "      <td>0.71599</td>\n",
       "      <td>0.71599</td>\n",
       "      <td>0.71599</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.636436</td>\n",
       "      <td>0.556881</td>\n",
       "      <td>0.477327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              expnum   match_es     shar_o       shar   numdat_2     length  \\\n",
       "missing %  77.724741  14.638027  12.967383  12.728719  11.296738  11.137629   \n",
       "\n",
       "             satis_2     amb_o       amb       fun     fun_o      met  \\\n",
       "missing %  11.137629  8.512331  8.035004  4.375497  4.295943  3.89817   \n",
       "\n",
       "             prob_o     prob   sinc_o   intel_o     intel    like_o      like  \\\n",
       "missing %  3.579952  3.18218  3.18218  2.943516  2.863962  2.704853  2.625298   \n",
       "\n",
       "               sinc      attr    attr_o  pf_o_sha     age_o  pf_o_amb  \\\n",
       "missing %  2.545744  1.988862  1.909308  1.909308  1.511535  1.431981   \n",
       "\n",
       "           pf_o_fun   attr3_1   sinc3_1    fun3_1  intel3_1    amb3_1  \\\n",
       "missing %  1.352426  1.272872  1.272872  1.272872  1.272872  1.272872   \n",
       "\n",
       "             race_o  pf_o_att  pf_o_sin  pf_o_int  career_c  exphappy  \\\n",
       "missing %  1.272872  1.272872  1.272872  1.272872  1.034208  0.875099   \n",
       "\n",
       "               date   shar1_1      age   amb1_1   fun1_1  intel1_1   sinc1_1  \\\n",
       "missing %  0.795545  0.795545  0.71599  0.71599  0.71599  0.636436  0.636436   \n",
       "\n",
       "            attr1_1  shopping      yoga       art     music   museums  \\\n",
       "missing %  0.636436  0.636436  0.636436  0.636436  0.636436  0.636436   \n",
       "\n",
       "            imprace  imprelig      goal    go_out  concerts  tvsports  \\\n",
       "missing %  0.636436  0.636436  0.636436  0.636436  0.636436  0.636436   \n",
       "\n",
       "           exercise    dining    sports    gaming  clubbing   reading  \\\n",
       "missing %  0.636436  0.636436  0.636436  0.636436  0.636436  0.636436   \n",
       "\n",
       "                 tv   theater    movies    hiking  field_cd      race  pid  \\\n",
       "missing %  0.636436  0.636436  0.636436  0.636436  0.556881  0.477327  0.0   \n",
       "\n",
       "           wave  position  order  gender  dec_o  met_o  dec  iid  \n",
       "missing %   0.0       0.0    0.0     0.0    0.0    0.0  0.0  0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dec'].isnull().any()\n",
    "((df.isna().sum()/len(df)) * 100).to_frame(name='missing %').sort_values(by=['missing %'], ascending=False).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce29f3",
   "metadata": {},
   "source": [
    "##### Imputate missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d1e296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = col_trans.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097ae9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_pipeline(test):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3cf6c",
   "metadata": {},
   "source": [
    "Some features have wrong data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c97aa",
   "metadata": {},
   "source": [
    "Verify that data type is changed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f249b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"Categorical Features\")\n",
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108efcf",
   "metadata": {},
   "source": [
    "#### 2. Columns renaming & dropping irrelevant columns\n",
    "\n",
    "It's better to rename columns (without white characters and preferably in _lower case_), so it's easier to deal with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f304651",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_frame().reset_index().T.head(1) # showing columns in a more visual way"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13b45e",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "\n",
    "- No need to change the column names since all are lower case and without any white characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb593b2",
   "metadata": {},
   "source": [
    "Based on our domain knowledge, we have identified relevant features and their associated datatypes. Let's drop the irrelevant features and update the data types again (just for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fbdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features = [\n",
    "    ['iid', 'int16'], ['gender', 'bool'],\n",
    "    ['wave', 'int16'], ['position', 'int16'],\n",
    "    ['order', 'int16'], ['pid', 'int16'],\n",
    "    ['age_o', 'int16'], ['race_o', 'category'],\n",
    "    ['pf_o_att', 'int16'], ['pf_o_sin', 'int16'],\n",
    "    ['pf_o_int', 'int16'], ['pf_o_fun', 'int16'],\n",
    "    ['pf_o_amb', 'int16'], ['pf_o_sha', 'int16'],\n",
    "    ['dec_o', 'bool'], ['attr_o', 'int16'], ['sinc_o', 'int16'], \n",
    "    ['intel_o', 'int16'], ['fun_o', 'int16'], ['amb_o', 'int16'], \n",
    "    ['shar_o', 'int16'], ['like_o', 'int16'],\n",
    "    ['prob_o', 'int16'], ['met_o', 'bool'], ['age', 'int16'], ['field_cd', 'category'], ['race', 'category'],\n",
    "    ['imprace', 'int16'], ['imprelig', 'int16'], ['goal', 'category'], ['date', 'int16'],\n",
    "    ['go_out', 'int16'], ['career_c', 'category'], ['sports', 'int16'], ['tvsports', 'int16'], ['exercise', 'int16'],\n",
    "    ['dining', 'int16'], ['museums', 'int16'], ['art', 'int16'], ['hiking', 'int16'],\n",
    "    ['gaming', 'int16'], ['clubbing', 'int16'], ['reading', 'int16'], ['tv', 'int16'],\n",
    "    ['theater', 'int16'], ['movies', 'int16'], ['concerts', 'int16'], ['music', 'int16'],\n",
    "    ['shopping', 'int16'], ['yoga', 'int16'], ['exphappy', 'int16'], ['expnum', 'int16'],\n",
    "    ['attr1_1', 'int16'], ['sinc1_1', 'int16'], ['intel1_1', 'int16'], ['fun1_1', 'int16'],\n",
    "    ['amb1_1', 'int16'], ['shar1_1', 'int16'], ['attr3_1', 'int16'], ['sinc3_1', 'int16'],\n",
    "    ['fun3_1', 'int16'], ['intel3_1', 'int16'], ['amb3_1', 'int16'], ['dec', 'bool'],\n",
    "    ['attr', 'int16'], ['sinc', 'int16'], ['intel', 'int16'], ['fun', 'int16'],\n",
    "    ['amb', 'int16'], ['shar', 'int16'], ['like', 'int16'], ['prob', 'int16'],\n",
    "    ['met', 'int16'], ['match_es', 'int16'], ['satis_2', 'int16'], ['length', 'int16'],\n",
    "    ['numdat_2', 'int16']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c8320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[feature[0] for feature in relevant_features]]\n",
    "df.shape\n",
    "df.memory_usage().sum() # memory usage in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f8f13",
   "metadata": {},
   "source": [
    "Let's update the data types for the relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({feature: datatype if all(df[feature].notna().values) else 'float32' if datatype == 'int16' else datatype for (feature, datatype) in relevant_features})\n",
    "df.dtypes.to_frame(name='data types').T # T will represent the transpose of the resulting dataframe, better for visualization\n",
    "df.shape\n",
    "df.memory_usage().sum() # memory usage in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13b45e",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "\n",
    "- After dropping irrelevant data and updating the datatypes, the dataframe size almost reduced by 82% the original size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bba4bd",
   "metadata": {},
   "source": [
    "#### 3. Split your dataset into train and test datasets\n",
    "\n",
    "We will split our dataset into two parts: `train` & `test` datasets. We will do all the processing on the `train` dataset. `test` dataset will remain unknown to us. And we will use it only for testing analysis. This is to simulate the real world scenario in which we don't know the data which would be run on our model (after deploying)\n",
    "\n",
    "However, there may be some issues with the `test` dataset (especially if our original dataset is bit imbalance):\n",
    "\n",
    "1. What if few categories are missed in `test` dataset ? we won't have any hot encoding for those categories.\n",
    "\n",
    "> One possible solution is to hot encode the new unseen categories as zero (or some default category)\n",
    "\n",
    "> Another possible solution is to do stratified sampling (in case of imbalance data), so you have data for all the categories in both `train` and `test` datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dec.replace(0, 'No').replace(1, 'Yes').value_counts().plot(kind = 'bar')\n",
    "dec_options = {False: \"No\", True: \"Yes\"}\n",
    "count_ss = df.dec.replace(dec_options).value_counts() # final decision. 1 = Yes, 0 = No\n",
    "ax = sns.barplot(x = count_ss.index, y = count_ss.values)\n",
    "_ = ax.set(xlabel='decision', ylabel='count')\n",
    "_ = ax.bar_label(ax.containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fca5b9c",
   "metadata": {},
   "source": [
    "Let's plot the distributions of subject attribute ratings from their partners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(16,10))\n",
    "_ = plt.tight_layout(pad=5.0)\n",
    "\n",
    "bins = np.arange(0, 10, 0.5).tolist()\n",
    "ylabel = \"No. of subjects\"\n",
    "\n",
    "cols = ['attr_o', 'sinc_o', 'intel_o', 'fun_o', 'amb_o', 'shar_o']\n",
    "xlabels = ['Attractiveness rating', 'Sincerity rating', 'Intelligence rating', 'Fun rating', 'Ambition rating', 'Shared interest rating']\n",
    "\n",
    "for i, col in enumerate(cols):\n",
    "    _ = plt.subplot(2,3, i + 1)\n",
    "    xlabel = xlabels[i]\n",
    "    plot_distribution(\n",
    "        data=df[col],\n",
    "        bins=bins,\n",
    "        title=\"Subject's {}\".format(xlabel),\n",
    "        xlabel=\"{}\".format(xlabel),\n",
    "        ylabel=ylabel\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbceec",
   "metadata": {},
   "source": [
    "We have a sightly imbalance data (for `decision` parameter). We will be doing a stratified split for equal proprtion in `train` and `test` datasets. You can check the official doc for `train_test_split` [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0567809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the random state too, so you we can reproduce the results later as well\n",
    "__random_state = 0\n",
    "\n",
    "# let's do a 85% | 15% split\n",
    "train_df, test_df = train_test_split(df, test_size=0.15, shuffle=True, random_state=__random_state, stratify=df['dec'])\n",
    "\n",
    "# reset the index for train and test\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59eec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_df.dec.replace(dec_options).value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_df.dec.replace(dec_options).value_counts().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66430aec",
   "metadata": {},
   "source": [
    "Both `train` and `test` datasets have the same ratio of values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b69f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.describe(include='all').T # check train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcfcc29",
   "metadata": {},
   "source": [
    "Save the `train` & `test` dataset as a `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa848134",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = join(data_dir, \"train.csv\")\n",
    "dataframe_to_csv(train_df, train_file_path)\n",
    "\n",
    "test_file_path = join(data_dir, \"test.csv\")\n",
    "dataframe_to_csv(test_df, test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd778c23",
   "metadata": {},
   "source": [
    "#### 4. Check for missing data\n",
    "\n",
    "We need to check for missing data and imputate or remove it. It is really important to deal with all the missing data to get better EDA and less incorrect results during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic first step is to check if any data is missing in predicted value\n",
    "# because if some labels are not there in y_train, there isn't any point to include those rows\n",
    "\n",
    "train_df['dec'].isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2df2a",
   "metadata": {},
   "source": [
    "So basically we are good here. Now let's see how much missing values we have for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_df.isna().sum()/len(train_df)) * 100).to_frame(name='missing %').sort_values(by=['missing %'], ascending=False).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e240c",
   "metadata": {},
   "source": [
    "Okay! so we have lots of columns with missing values (_as also already shown in EDA too_). Let's see what are the typical ways to deal with them\n",
    "\n",
    "There are multiple ways to compute the missing values. Although most of them are `distance based` and only work on _numeric_ features. Missing values are computed on training data only otherwise it can cause _Data Leakage_ (accidentally share the information between the `test` and `train` datasets) as well. List below are the typical ways we use to deal with missing values:\n",
    "\n",
    "1. Check manually what values are missing and correct them. We can check if there is a pattern to the missing values, think about the possible reasons why it happened in the first place\n",
    "\n",
    "2. Also, it is know practice to drop the columns if they have more than ~50% of the missing values\n",
    "\n",
    "3. Deleting rows with missing values - Deleting rows (if we have enough data) or column (if column is not important intutive idea from domain knowledge)\n",
    "\n",
    "4. Impute missing values for continuous variable - mean/median - without causing _data leakage_\n",
    "\n",
    "5. Impute missing values for categorical variable - mode ('most_frequent' or 'constant' strategy)\n",
    "\n",
    "6. Other Imputation Methods - interpolation (provide missing term using nearby trends, newton forward/backward formula, for multivariate - nearest neighbour interpolation, gaussian, polynomial (good for time series interpolation))\n",
    "\n",
    "7. Using Algorithms that support missing values (k-NN - based on k, get the closest rows using euclidean distance and take the average of there missing value column values)\n",
    "\n",
    "> Note: Imputation: Replace missing datas with statistical values. sklearn has a [`Imputation` module (sklearn.impute)](https://scikit-learn.org/stable/modules/impute.html#impute) which is quite helpful here.\n",
    "\n",
    "#### How to impute the missing data ? What do we need ?\n",
    "\n",
    "- A generalized model for missing values so that if values are missing in test data the model will not break\n",
    "- Although the mordern tree based algorithms manages the missing values by default using techniques such as `fragments` and `surrogate splits` so while using these algos we do not need to worry about missing values\n",
    "- We can make a gernalized imputation model which can deal with all the unexpected missing values if required\n",
    "\n",
    "---\n",
    "\n",
    "#### Some of the imputers being used from [_sklearn.impute_](https://scikit-learn.org/stable/modules/impute.html#impute) module\n",
    "\n",
    "##### [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer)\n",
    "\n",
    "- Replace `NaN` (`np.nan`) by the `mean`, `median`, `most_frequent` or `constant`\n",
    "\n",
    "\n",
    "##### [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer)\n",
    "\n",
    "- Replace your missing values by closest ones\n",
    "- Imputation for completing missing values using k-Nearest Neighbors\n",
    "- Each samples missing values are imputed using the _mean_ value from `n_neighbors` nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close\n",
    "\n",
    "##### [MissingIndicator](https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator)\n",
    "\n",
    "- Position of missing values by a `boolean` mask\n",
    "\n",
    "##### [IterativeImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)\n",
    "\n",
    "- Multivariate imputer that estimates each feature from all the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold for missing value - remove all columns which has greater than 50% of missing values\n",
    "__missing_threshold = float(50)\n",
    "\n",
    "missing_df = ((train_df.isna().sum()/len(train_df)) * 100).to_frame(name='missing').sort_values(by=['missing'], ascending=False).query(\"missing > {}\".format(__missing_threshold))\n",
    "missing_cols = missing_df.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputate missing values\n",
    "def iterative_imputate_missing_features(data, random_state = 0, relevant_features=None):\n",
    "    \"\"\"\n",
    "    Method to imputate missing values using IterativeImputer\n",
    "    \"\"\"\n",
    "    imputer = IterativeImputer(\n",
    "        missing_values=np.nan,\n",
    "        sample_posterior=True, # sample from gaussian predictive posterior\n",
    "        n_nearest_features=5,\n",
    "        min_value=0,\n",
    "        max_value=100,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    imputer.fit(data)\n",
    "    data_imputed = np.around(imputer.transform(data))\n",
    "    data = pd.DataFrame(data_imputed, columns=data.columns)\n",
    "    if relevant_features:\n",
    "        data = data.astype({feature: datatype if all(data[feature].notna().values) else 'float32' if datatype == 'int16' else datatype for (feature, datatype) in relevant_features})\n",
    "    return data, imputer\n",
    "\n",
    "def imputate_missing_features(data, missing_threshold = 50):\n",
    "    \"\"\"\n",
    "    Method to imputate missing values and return a imputate model\n",
    "    \"\"\"\n",
    "\n",
    "    # threshold for missing value - remove all columns which has greater than 50% of missing values\n",
    "    __missing_threshold = missing_threshold\n",
    "\n",
    "    missing_df = ((data.isna().sum()/len(data)) * 100).to_frame(name='missing').sort_values(by=['missing'], ascending=False).query(\"missing > {}\".format(__missing_threshold))\n",
    "    missing_cols = missing_df.index.to_list()\n",
    "\n",
    "    cols_to_use = list(set(data.columns.to_list()) - set(missing_cols))\n",
    "    cols_to_use = [x for x in cols_to_use if x != 'dec']\n",
    "    \n",
    "    #X, y = data[cols_to_use], data['dec']\n",
    "    X, y = data, data['dec']\n",
    "    X.drop(['dec'], axis=1, inplace=True)\n",
    "\n",
    "    num_features = make_column_selector(dtype_include=np.number) # get all numeric data\n",
    "    cat_features = make_column_selector(dtype_exclude=np.number)\n",
    "\n",
    "    imputate_pipeline = make_column_transformer(\n",
    "        (SimpleImputer(strategy='mean'), num_features),\n",
    "        (MissingIndicator(missing_values=np.nan, features=\"all\", error_on_new=False), num_features),\n",
    "\n",
    "        remainder=\"drop\", # drop the remaining columns\n",
    "        n_jobs=-1, # run jobs using all available processors (for speedup computation)\n",
    "    )\n",
    "\n",
    "    num_pipe = make_pipeline(\n",
    "        KNNImputer(n_neighbors=5, add_indicator=True),    # stack MissingIndicator on the output\n",
    "        #imputate_pipeline,\n",
    "        StandardScaler()\n",
    "    )\n",
    "\n",
    "    cat_pipe = make_pipeline(\n",
    "        SimpleImputer(strategy = 'most_frequent'),\n",
    "        OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    )\n",
    "\n",
    "    col_trans = make_column_transformer(\n",
    "        (num_pipe, num_features),\n",
    "        (cat_pipe, cat_features)\n",
    "    )\n",
    "\n",
    "    col_trans.fit(X)\n",
    "\n",
    "    return X, cols_to_use, col_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8a7888",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, col_trans = iterative_imputate_missing_features(train_df, __random_state, relevant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape\n",
    "col_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, cols_to_use, col_trans = imputate_missing_features(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d440f",
   "metadata": {},
   "source": [
    "Save the imputated model as pickle (`.pkl`) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93a969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(col_trans, join(model_dir, \"col_trans.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835479e6",
   "metadata": {},
   "source": [
    "Imputate the `train` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef46337",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().any() # check missing values now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95db9c2",
   "metadata": {},
   "source": [
    "#### 5. Check for outliers\n",
    "\n",
    "In order to check outliers in your dataset, there are various methods worth looking into:\n",
    "\n",
    "1. _Box plots_\n",
    "2. _Z-score test (normal distribution assumption)_\n",
    "3. _Model based (One class SVM, density based algorithams, etc)_\n",
    "\n",
    "Some useful resources:\n",
    "\n",
    "- [Novelty and Outlier Detection](https://scikit-learn.org/stable/modules/outlier_detection.html)\n",
    "- [Ways to Detect and Remove the Outliers](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n",
    "\n",
    "> For our use case, we will be checking outliers with _box plots_ and _density based algoritham_ (`DBSCAN`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c1aef7",
   "metadata": {},
   "source": [
    "##### Box Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box_plots(df, cols):\n",
    "    \"\"\"\n",
    "    Draw Box Plot & Histogram for each column\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))\n",
    "        _ = df.boxplot(column=col,ax=axes[0]);\n",
    "        _ = df.hist(column=col, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca5441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all numeric columns\n",
    "train_numeric = train_df.select_dtypes(include=\"number\")\n",
    "draw_box_plots(train_df, train_numeric.columns)\n",
    "# train_numeric.shape\n",
    "# train_numeric.head()\n",
    "# total_num_cols = len(train_numeric.columns)\n",
    "# for col in train_numeric.columns:\n",
    "#     fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))\n",
    "#     _ = train_df.boxplot(column=col,ax=axes[0]);\n",
    "#     _ = train_df.hist(column=col, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2501dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(df, col, mode=1, times_std=3):\n",
    "    \"\"\"\n",
    "    Basic way to remove outliers\n",
    "\n",
    "    mode = 1 (via mean and std) \n",
    "    mode = 2 (via IQR)\n",
    "    \"\"\"\n",
    "    if mode == 1: # with mean and std\n",
    "        upper_limit = df[col].mean() + times_std * df[col].std()\n",
    "        lower_limit = df[col].mean() - times_std * df[col].std()\n",
    "        df[col] = np.where(\n",
    "            df[col] > upper_limit,\n",
    "            upper_limit,\n",
    "            np.where(\n",
    "                df[col] < lower_limit,\n",
    "                lower_limit,\n",
    "                df[col]\n",
    "            )\n",
    "        )\n",
    "    elif mode == 2:\n",
    "\n",
    "        p_25, p_75 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
    "        iqr = p_75 - p_25\n",
    "        upper_limit = p_75 + 1.5 * iqr\n",
    "        lower_limit = p_25 - 1.5 * iqr\n",
    "\n",
    "        df[col] = np.where(\n",
    "            df[col] > upper_limit,\n",
    "            upper_limit,\n",
    "            np.where(\n",
    "                df[col] < lower_limit,\n",
    "                lower_limit,\n",
    "                df[col]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        print(\"Unsupported mode\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974731c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = remove_outlier(train_df, train_numeric.columns, mode=2)\n",
    "#draw_box_plots(train_df, train_numeric.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380382e3",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "\n",
    "- Removed all the outliers from the data\n",
    "\n",
    "---\n",
    "\n",
    "#### Clustering Algorithm\n",
    "\n",
    "##### [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)\n",
    "\n",
    "- DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c282b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the attribute using domain knowledge\n",
    "# outlier_detection = DBSCAN(min_samples = 4, eps = 0.5)\n",
    "\n",
    "# make clusters\n",
    "# normalized_df=(train_numeric - train_numeric.min()) / (train_numeric.max() - train_numeric.min())\n",
    "# clusters = outlier_detection.fit_predict(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e785eaf3",
   "metadata": {},
   "source": [
    "#### 6. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a9d9ee",
   "metadata": {},
   "source": [
    "Encode nominal features using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b6338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nominal = train_df.dtypes[train_df.dtypes == 'category'].index.values\n",
    "train_df = pd.get_dummies(train_df, prefix=features_nominal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea132989",
   "metadata": {},
   "source": [
    "Calculate the average attribute ratings for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df.copy()\n",
    "\n",
    "subject_attractiveness_mean = data[['iid', 'attr_o']].groupby(['iid']).mean()['attr_o']\n",
    "subject_sincerity_mean = data[['iid', 'sinc_o']].groupby(['iid']).mean()['sinc_o']\n",
    "subject_intelligence_mean = data[['iid', 'intel_o']].groupby(['iid']).mean()['intel_o']\n",
    "subject_fun_mean = data[['iid', 'fun_o']].groupby(['iid']).mean()['fun_o']\n",
    "subject_ambition_mean = data[['iid', 'amb_o']].groupby(['iid']).mean()['amb_o']\n",
    "subject_shared_interest_mean = data[['iid', 'shar_o']].groupby(['iid']).mean()['shar_o']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262a0faa",
   "metadata": {},
   "source": [
    "Insert average attribute ratings into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f17899",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(\n",
    "    right=subject_attractiveness_mean,\n",
    "    how='inner',\n",
    "    on='iid'\n",
    ").rename(columns={\n",
    "    'attr_o_x': 'attr_o',\n",
    "    'attr_o_y': 'subject_attractiveness_mean'\n",
    "})\n",
    "data = data.merge(\n",
    "    right=subject_sincerity_mean,\n",
    "    how='inner',\n",
    "    on='iid'\n",
    ").rename(columns={\n",
    "    'sinc_o_x': 'sinc_o',\n",
    "    'sinc_o_y': 'subject_sincerity_mean'\n",
    "})\n",
    "data = data.merge(\n",
    "    right=subject_intelligence_mean,\n",
    "    how='inner',\n",
    "    on='iid'\n",
    ").rename(columns={\n",
    "    'intel_o_x': 'intel_o',\n",
    "    'intel_o_y': 'subject_intelligence_mean'\n",
    "})\n",
    "data = data.merge(\n",
    "    right=subject_fun_mean,\n",
    "    how='inner',\n",
    "    on='iid'\n",
    ").rename(columns={\n",
    "    'fun_o_x': 'fun_o',\n",
    "    'fun_o_y': 'subject_fun_mean'\n",
    "})\n",
    "data = data.merge(\n",
    "    right=subject_ambition_mean,\n",
    "    how='inner',\n",
    "    on='iid'\n",
    ").rename(columns={\n",
    "    'amb_o_x': 'amb_o',\n",
    "    'amb_o_y': 'subject_ambition_mean'\n",
    "})\n",
    "data = data.merge(\n",
    "    right=subject_shared_interest_mean,\n",
    "    how='inner',\n",
    "    on='iid'\n",
    ").rename(columns={\n",
    "    'shar_o_x': 'shar_o',\n",
    "    'shar_o_y': 'subject_shared_interest_mean'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5dd4d",
   "metadata": {},
   "source": [
    "Calculate difference between subject and partner's ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e92bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age_difference'] = abs(data['age'] - data['age_o'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08104db",
   "metadata": {},
   "source": [
    "Calculate difference between subject's attribute ratings and partner's attributes ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d13bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['attractiveness_difference'] = abs(data['attr'] - data['attr_o'])\n",
    "data['sincerity_difference'] = abs(data['sinc'] - data['sinc_o'])\n",
    "data['intelligence_difference'] = abs(data['intel'] - data['intel_o'])\n",
    "data['fun_difference'] = abs(data['fun'] - data['fun_o'])\n",
    "data['ambition_difference'] = abs(data['amb'] - data['amb_o'])\n",
    "data['shared_interest_difference'] = abs(data['shar'] - data['shar_o'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d7c11",
   "metadata": {},
   "source": [
    "Scale normal features to zero mean and unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8fb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_normal = [\n",
    "    'attr_o',\n",
    "    'sinc_o',\n",
    "    'intel_o',\n",
    "    'fun_o',\n",
    "    'amb_o',\n",
    "    'shar_o',\n",
    "    'age_difference',\n",
    "    'attractiveness_difference',\n",
    "    'sincerity_difference',\n",
    "    'intelligence_difference',\n",
    "    'fun_difference',\n",
    "    'ambition_difference',\n",
    "    'shared_interest_difference'\n",
    "]\n",
    "\n",
    "data[features_normal] = data[features_normal].apply(lambda x: preprocessing.scale(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0341f9",
   "metadata": {},
   "source": [
    "Drop some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971328f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant features which contain no information about the target variable\n",
    "features_no_information = [\n",
    "    'iid',\n",
    "    'pid',\n",
    "    'wave',\n",
    "    'position',\n",
    "    'order'\n",
    "]\n",
    "# Drop features that are known in the future\n",
    "features_future_information = [\n",
    "    'dec',\n",
    "    #'dec_o',\n",
    "    'like',\n",
    "    'prob',\n",
    "    'like_o',\n",
    "    'prob_o'\n",
    "]\n",
    "\n",
    "# Drop features that have low variance\n",
    "feature_variances = data.std().sort_values(ascending=True)\n",
    "features_low_variance = feature_variances[feature_variances < 0.1].index.values.tolist()\n",
    "\n",
    "# Drop features that have weak correlation with target variable\n",
    "correlations = data.corr().abs().unstack().sort_values(ascending=False).drop_duplicates()\n",
    "correlations = correlations[correlations != 1]\n",
    "partner_decision_correlations = correlations.loc['dec_o']\n",
    "features_weak_correlation = partner_decision_correlations[partner_decision_correlations < 0.1].axes[0].to_list()\n",
    "features_weak_correlation = list(set(features_weak_correlation) - set(features_future_information) - set(features_no_information))\n",
    "\n",
    "# Drop features that were used in interaction variables\n",
    "features_interaction = [\n",
    "    'age',\n",
    "    'age_o',\n",
    "]\n",
    "\n",
    "features_remove = features_no_information + features_future_information + features_low_variance + features_weak_correlation + features_interaction\n",
    "data.drop(columns=features_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5340eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "data.memory_usage().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13b45e",
   "metadata": {},
   "source": [
    "##### Comment\n",
    "\n",
    "- After dropping more irrelevant data, the dataframe size has further decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b0dd30",
   "metadata": {},
   "source": [
    "Save this pre-processed data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_file_path = join(data_dir, \"preprocessed.csv\")\n",
    "dataframe_to_csv(data, preprocessed_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f939fc",
   "metadata": {},
   "source": [
    "Below mentioned are the articles, blogs, papers and other resources which we took inspiration from or we think are useful for future use case\n",
    "\n",
    "#### References\n",
    "\n",
    "- [_Getting Started with Data Preprocessing in Python_](https://www.section.io/engineering-education/data-preprocessing-python/)\n",
    "- [_Data Preprocessing in Python_](https://medium.datadriveninvestor.com/data-preprocessing-3cd01eefd438)\n",
    "- [_Data Pre-processing in Python for Beginner_](https://medium.com/data-science-indo/data-preparation-in-python-for-beginner-d3e1e60c03a6)\n",
    "- [_Data Preprocessing using Python_](https://medium.com/@suneet.bhopal/data-preprocessing-using-python-1bfee9268fb3)\n",
    "- [_Data preprocessing for Machine Learning in Python_](https://towardsdatascience.com/data-preprocessing-for-machine-learning-in-python-2d465f83f18c)\n",
    "- [_Data Preprocessing in Machine Learning_](https://www.analytixlabs.co.in/blog/data-preprocessing-in-machine-learning/)\n",
    "- [_Normalization vs Standardization  Quantitative analysis_](https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf)\n",
    "- [__]()\n",
    "- [__]()\n",
    "- [__]()\n",
    "- [__]()\n",
    "- [__]()\n",
    "- [__]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682e7ae",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b33028dfef90447248446bd6a115b2c1f87c179fa7c4ab6e59f7d48c9bbeef80"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
